{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer encoder for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:44:02.309839600Z",
     "start_time": "2023-05-22T08:43:59.914473504Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 10:44:00.483470: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/write/count\n",
      "2023-05-22 10:44:00.483584: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/read/count\n",
      "2023-05-22 10:44:00.483612: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/write/api\n",
      "2023-05-22 10:44:00.483625: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/read/api\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import os, pathlib, shutil, random\n",
    "import shutil\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Imdb for sentences binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "\n",
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
    "                                    untar=True, cache_dir='.',\n",
    "                                    cache_subdir='')\n",
    "\n",
    "shutil.rmtree('aclImdb/train/unsup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validation set with the 20% of training data\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files) \n",
    "    num_val_samples = int(0.2 * len(files)) \n",
    "    val_files = files[-num_val_samples:] \n",
    "    for fname in val_files: \n",
    "        shutil.move(train_dir / category / fname, val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:44:09.608392189Z",
     "start_time": "2023-05-22T08:44:03.099663090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 10:44:04.521617: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 10:44:06.477920: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:164] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "# Prepare datasets that return integers sequences\n",
    "\n",
    "batch_size = 32\n",
    "train_ds = keras.utils.text_dataset_from_directory( \"aclImdb/train\", batch_size=batch_size)\n",
    "val_ds = keras.utils.text_dataset_from_directory(\"aclImdb/val\", batch_size=batch_size)\n",
    "test_ds = keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size=batch_size)\n",
    "\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x) \n",
    "\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = TextVectorization(\n",
    " max_tokens=max_tokens,\n",
    " output_mode=\"int\",\n",
    " output_sequence_length=max_length, \n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(int_test_ds)[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim \n",
    "        self.dense_dim = dense_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential([layers.Dense(dense_dim, activation=\"relu\"),layers.Dense(embed_dim),])\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None): \n",
    "        if mask is not None: \n",
    "            mask = mask[:, tf.newaxis, :] \n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "    \n",
    "    def get_config(self): \n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "        \"embed_dim\": self.embed_dim,\n",
    "        \"num_heads\": self.num_heads,\n",
    "        \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs): \n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=output_dim) \n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "    \n",
    "    def compute_mask(self, inputs, mask=None): \n",
    "        return tf.math.not_equal(inputs, 0) \n",
    "    \n",
    "    def get_config(self): \n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "        \"output_dim\": self.output_dim,\n",
    "        \"sequence_length\": self.sequence_length,\n",
    "        \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "sequence_length = 600\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs) \n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"adam\",\n",
    " loss=\"binary_crossentropy\",\n",
    " metrics=[tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "625/625 [==============================] - 351s 561ms/step - loss: 0.4607 - binary_accuracy: 0.7852 - val_loss: 0.2795 - val_binary_accuracy: 0.8856\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - 343s 549ms/step - loss: 0.1673 - binary_accuracy: 0.9359 - val_loss: 0.3388 - val_binary_accuracy: 0.8702\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - 342s 548ms/step - loss: 0.0798 - binary_accuracy: 0.9718 - val_loss: 0.4555 - val_binary_accuracy: 0.8672\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 345s 553ms/step - loss: 0.0548 - binary_accuracy: 0.9805 - val_loss: 0.4925 - val_binary_accuracy: 0.8638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f961eefbb50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"encoder_model/full_transformer_encoder.h5\", save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "] \n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, \n",
    "callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\n",
    "    \"encoder_model/full_transformer_encoder.h5\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
    "                    \"PositionalEmbedding\": PositionalEmbedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights = model.get_weights()\n",
    "flattened_weights = [w.flatten() for w in weights]\n",
    "weights_array = [item for sublist in flattened_weights for item in sublist]\n",
    "sns.histplot(weights_array, stat='frequency', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "positional_embedding (Positi (None, None, 256)         5273600   \n",
      "_________________________________________________________________\n",
      "transformer_encoder (Transfo (None, None, 256)         543776    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 5,817,633\n",
      "Trainable params: 5,817,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model accuracy with Float32 default weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 157s 201ms/step - loss: 0.3013 - binary_accuracy: 0.8748\n",
      "Test acc: 0.875\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 25s 198ms/step - loss: 0.2917 - binary_accuracy: 0.8796\n",
      "Test acc: 0.880\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test acc: {model.evaluate(int_test_ds, steps=128)[1]:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model accuracy converting the weights to Float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dtype('float32')]\n"
     ]
    }
   ],
   "source": [
    "K.set_floatx('float16')\n",
    "\n",
    "# Get the original weights\n",
    "ws = model.get_weights()\n",
    "print(np.unique([w.dtype for w in model.get_weights()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dtype('float16')]\n"
     ]
    }
   ],
   "source": [
    "# Convert the weights to Posit <16,0> and load a new model\n",
    "wsp = [w.astype(K.floatx()) for w in ws]\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs) \n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_float16 = keras.Model(inputs, outputs)\n",
    "model_float16.compile(optimizer=\"adam\",\n",
    " loss=\"binary_crossentropy\",\n",
    " metrics=[\"accuracy\"])\n",
    "model_float16.set_weights(wsp)\n",
    "\n",
    "print(np.unique([w.dtype for w in model_float16.get_weights()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 2373s 19s/step - loss: 0.2893 - accuracy: 0.8789\n",
      "Test acc: 0.879\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test acc: {model_float16.evaluate(int_test_ds, steps=128)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_float16.save(\"encoder_model/test_float16.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model accuracy converting the weights to Posit<16,0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dtype('float32')]\n"
     ]
    }
   ],
   "source": [
    "K.set_floatx('posit160')\n",
    "\n",
    "# Get the original weights\n",
    "ws = model.get_weights()\n",
    "print(np.unique([w.dtype for w in model.get_weights()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dtype(posit160)]\n"
     ]
    }
   ],
   "source": [
    "# Convert the weights to Posit <16,0> and load a new model\n",
    "wsp = [w.astype(K.floatx()) for w in ws]\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs) \n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_posit = keras.Model(inputs, outputs)\n",
    "model_posit.compile(optimizer=\"adam\",\n",
    " loss=\"binary_crossentropy\",\n",
    " metrics=[\"accuracy\"])\n",
    "model_posit.set_weights(wsp)\n",
    "\n",
    "print(np.unique([w.dtype for w in model_posit.get_weights()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 7244s 57s/step - loss: 0.3354 - accuracy: 0.8525\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test acc: {model_posit.evaluate(int_test_ds, steps=128)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_posit.save(\"encoder_model/test_posit160.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model converting the input from int to posit<16,0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 600), dtype=posit160, numpy=\n",
      "array([[503, 19, 52, ..., 0, 0, 0],\n",
      "       [10, 274, 11, ..., 0, 0, 0],\n",
      "       [10, 472, 10, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [58, 58, 58, ..., 0, 0, 0],\n",
      "       [1, 7, 230, ..., 0, 0, 0],\n",
      "       [10, 90, 449, ..., 0, 0, 0]], dtype=posit160)>, <tf.Tensor: shape=(32,), dtype=posit160, numpy=\n",
      "array([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
      "       0, 0, 0, 1, 1, 1, 1, 0, 0, 1], dtype=posit160)>)\n"
     ]
    }
   ],
   "source": [
    "int_test_ds = int_test_ds.map(lambda x, y: (tf.cast(x, tf.posit160), tf.cast(y, tf.posit160)))\n",
    "print(list(int_test_ds)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 156s 200ms/step - loss: 0.4179 - binary_accuracy: 0.8120\n",
      "Test acc: 0.812\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 26s 205ms/step - loss: 0.4195 - binary_accuracy: 0.8066\n",
      "Test acc: 0.807\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test acc: {model.evaluate(int_test_ds, steps=128)[1]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
