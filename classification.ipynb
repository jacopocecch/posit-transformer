{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer encoder for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:44:02.309839600Z",
     "start_time": "2023-05-22T08:43:59.914473504Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 10:44:00.483470: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/write/count\n",
      "2023-05-22 10:44:00.483584: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/read/count\n",
      "2023-05-22 10:44:00.483612: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/write/api\n",
      "2023-05-22 10:44:00.483625: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/read/api\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import os, pathlib, shutil, random\n",
    "import shutil"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Imdb for sentences binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "\n",
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
    "                                    untar=True, cache_dir='.',\n",
    "                                    cache_subdir='')\n",
    "\n",
    "shutil.rmtree('aclImdb/train/unsup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validation set with the 20% of training data\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files) \n",
    "    num_val_samples = int(0.2 * len(files)) \n",
    "    val_files = files[-num_val_samples:] \n",
    "    for fname in val_files: \n",
    "        shutil.move(train_dir / category / fname, val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:44:09.608392189Z",
     "start_time": "2023-05-22T08:44:03.099663090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 10:44:04.521617: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 10:44:06.477920: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:164] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "# Prepare datasets that return integers sequences\n",
    "\n",
    "batch_size = 32\n",
    "train_ds = keras.utils.text_dataset_from_directory( \"aclImdb/train\", batch_size=batch_size)\n",
    "val_ds = keras.utils.text_dataset_from_directory(\"aclImdb/val\", batch_size=batch_size)\n",
    "test_ds = keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size=batch_size)\n",
    "\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x) \n",
    "\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = TextVectorization(\n",
    " max_tokens=max_tokens,\n",
    " output_mode=\"int\",\n",
    " output_sequence_length=max_length, \n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(int_test_ds)[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim \n",
    "        self.dense_dim = dense_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential([layers.Dense(dense_dim, activation=\"relu\"),layers.Dense(embed_dim),])\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None): \n",
    "        if mask is not None: \n",
    "            mask = mask[:, tf.newaxis, :] \n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "    \n",
    "    def get_config(self): \n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "        \"embed_dim\": self.embed_dim,\n",
    "        \"num_heads\": self.num_heads,\n",
    "        \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs): \n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=output_dim) \n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "    \n",
    "    def compute_mask(self, inputs, mask=None): \n",
    "        return tf.math.not_equal(inputs, 0) \n",
    "    \n",
    "    def get_config(self): \n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "        \"output_dim\": self.output_dim,\n",
    "        \"sequence_length\": self.sequence_length,\n",
    "        \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "sequence_length = 600\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs) \n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"adam\",\n",
    " loss=\"binary_crossentropy\",\n",
    " metrics=[tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "625/625 [==============================] - 351s 561ms/step - loss: 0.4607 - binary_accuracy: 0.7852 - val_loss: 0.2795 - val_binary_accuracy: 0.8856\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - 343s 549ms/step - loss: 0.1673 - binary_accuracy: 0.9359 - val_loss: 0.3388 - val_binary_accuracy: 0.8702\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - 342s 548ms/step - loss: 0.0798 - binary_accuracy: 0.9718 - val_loss: 0.4555 - val_binary_accuracy: 0.8672\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 345s 553ms/step - loss: 0.0548 - binary_accuracy: 0.9805 - val_loss: 0.4925 - val_binary_accuracy: 0.8638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f961eefbb50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"encoder_model/full_transformer_encoder.h5\", save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "] \n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, \n",
    "callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\n",
    "    \"encoder_model/full_transformer_encoder.h5\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
    "                    \"PositionalEmbedding\": PositionalEmbedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "positional_embedding (Positi (None, None, 256)         5273600   \n",
      "_________________________________________________________________\n",
      "transformer_encoder (Transfo (None, None, 256)         543776    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 5,817,633\n",
      "Trainable params: 5,817,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model accuracy with Float32 default weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 157s 201ms/step - loss: 0.3013 - binary_accuracy: 0.8748\n",
      "Test acc: 0.875\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 25s 198ms/step - loss: 0.2917 - binary_accuracy: 0.8796\n",
      "Test acc: 0.880\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test acc: {model.evaluate(int_test_ds, steps=128)[1]:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model accuracy converting the weights to Float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dtype('float32')]\n"
     ]
    }
   ],
   "source": [
    "K.set_floatx('float16')\n",
    "\n",
    "# Get the original weights\n",
    "ws = model.get_weights()\n",
    "print(np.unique([w.dtype for w in model.get_weights()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dtype('float16')]\n"
     ]
    }
   ],
   "source": [
    "# Convert the weights to Posit <16,0> and load a new model\n",
    "wsp = [w.astype(K.floatx()) for w in ws]\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs) \n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_float16 = keras.Model(inputs, outputs)\n",
    "model_float16.compile(optimizer=\"adam\",\n",
    " loss=\"binary_crossentropy\",\n",
    " metrics=[\"accuracy\"])\n",
    "model_float16.set_weights(wsp)\n",
    "\n",
    "print(np.unique([w.dtype for w in model_float16.get_weights()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 2373s 19s/step - loss: 0.2893 - accuracy: 0.8789\n",
      "Test acc: 0.879\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test acc: {model_float16.evaluate(int_test_ds, steps=128)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_float16.save(\"encoder_model/test_float16.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model accuracy converting the weights to Posit<16,0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dtype('float32')]\n"
     ]
    }
   ],
   "source": [
    "K.set_floatx('posit160')\n",
    "\n",
    "# Get the original weights\n",
    "ws = model.get_weights()\n",
    "print(np.unique([w.dtype for w in model.get_weights()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dtype(posit160)]\n"
     ]
    }
   ],
   "source": [
    "# Convert the weights to Posit <16,0> and load a new model\n",
    "wsp = [w.astype(K.floatx()) for w in ws]\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs) \n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_posit = keras.Model(inputs, outputs)\n",
    "model_posit.compile(optimizer=\"adam\",\n",
    " loss=\"binary_crossentropy\",\n",
    " metrics=[\"accuracy\"])\n",
    "model_posit.set_weights(wsp)\n",
    "\n",
    "print(np.unique([w.dtype for w in model_posit.get_weights()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 7244s 57s/step - loss: 0.3354 - accuracy: 0.8525\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test acc: {model_posit.evaluate(int_test_ds, steps=128)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_posit.save(\"encoder_model/test_posit160.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model converting the input from int to posit<16,0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 600), dtype=posit160, numpy=\n",
      "array([[503, 19, 52, ..., 0, 0, 0],\n",
      "       [10, 274, 11, ..., 0, 0, 0],\n",
      "       [10, 472, 10, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [58, 58, 58, ..., 0, 0, 0],\n",
      "       [1, 7, 230, ..., 0, 0, 0],\n",
      "       [10, 90, 449, ..., 0, 0, 0]], dtype=posit160)>, <tf.Tensor: shape=(32,), dtype=posit160, numpy=\n",
      "array([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
      "       0, 0, 0, 1, 1, 1, 1, 0, 0, 1], dtype=posit160)>)\n"
     ]
    }
   ],
   "source": [
    "int_test_ds = int_test_ds.map(lambda x, y: (tf.cast(x, tf.posit160), tf.cast(y, tf.posit160)))\n",
    "print(list(int_test_ds)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 156s 200ms/step - loss: 0.4179 - binary_accuracy: 0.8120\n",
      "Test acc: 0.812\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 26s 205ms/step - loss: 0.4195 - binary_accuracy: 0.8066\n",
      "Test acc: 0.807\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test acc: {model.evaluate(int_test_ds, steps=128)[1]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
